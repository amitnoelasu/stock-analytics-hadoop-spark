# ğŸ“ˆ Stock Market ETL Pipeline (PySpark + Docker)

A modular, production-style **PySpark ETL pipeline** that ingests raw stock price data (CSV), enriches it with metadata, computes technical indicators (moving averages, volatility, returns), and writes the results as **partitioned Parquet datasets**.

The project is fully containerized using Docker and includes optional integration with **HDFS and Hive** for distributed storage/queries.

Data source: https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset?resource=download

---

## ğŸš€ Features

### âœ” Extract
- Automatically loads all ticker CSV files  
- Parses dates, normalizes schema  
- Loads NASDAQ metadata  

### âœ” Transform
- Computes:
  - Daily returns  
  - Rolling volatility (20-day)  
  - Moving averages (MA20, MA50)  
- Joins prices with metadata  
- Cleans up column types  

### âœ” Load
- Writes cleaned, enriched datasets as **partitioned Parquet**  
- Supports:
  - Local filesystem  
  - **HDFS** (`hdfs://â€¦`)  
  - Optional Hive table creation  

### âœ” Built-in SQL Queries
After ETL, Spark runs demonstration SQL queries for exploration.

---

## ğŸ“‚ Project Structure

â””â”€â”€ ğŸ“etl
        â””â”€â”€ ğŸ“extract
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ metadata_loader.py
            â”œâ”€â”€ prices_loader.py
        â””â”€â”€ ğŸ“load
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ writer.py
        â””â”€â”€ ğŸ“sql
            â”œâ”€â”€ query_parquet.py
        â””â”€â”€ ğŸ“transform
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ enrich.py
            â”œâ”€â”€ prices_transform.py
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ cli.py
        â”œâ”€â”€ config.py
        â”œâ”€â”€ pipeline.py
        â”œâ”€â”€ spark_app.py
    â””â”€â”€ ğŸ“notebooks
    â””â”€â”€ ğŸ“test_data
        â””â”€â”€ ğŸ“clean
            â””â”€â”€ ğŸ“prices_test (generated folder)
                â””â”€â”€ ğŸ“symbol=A
                    â”œâ”€â”€ .part-00000-c4021f04-770f-49c0-83eb-5fe7eac4da81.c000.snappy.parquet.crc
                    â”œâ”€â”€ part-00000-c4021f04-770f-49c0-83eb-5fe7eac4da81.c000.snappy.parquet
                    ....
        â””â”€â”€ ğŸ“raw
            â””â”€â”€ ğŸ“meta
                â”œâ”€â”€ symbols_valid_meta.csv
            â””â”€â”€ ğŸ“stocks
                â”œâ”€â”€ A.csv
                â”œâ”€â”€ AA.csv
                â”œâ”€â”€ AACG.csv
                â”œâ”€â”€ AAL.csv
                â”œâ”€â”€ AAMC.csv
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ run_etl.py # Entrypoint


### Input data format
Date,Open,High,Low,Close,Adj Close,Volume
2024-01-02,189.98,190.85,187.20,189.00,189.00,45000000

### Metadata CSV example
Symbol,Security Name,Listing Exchange,Market Category,ETF,...
AAPL,Apple Inc,Nasdaq Global Select,Q,...

### Output format (Parquet)
prices_enriched/
â””â”€â”€ symbol=AAPL/
      part-xxxxx.snappy.parquet
â””â”€â”€ symbol=AMZN/
â””â”€â”€ symbol=MSFT/

### Output path for test-data: /test_data/clean


